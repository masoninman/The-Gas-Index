{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gas flows model: gross trade between jurisdictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for iterative attribution process\n",
    "max_iter = 5000\n",
    "threshold_fract = 0.99\n",
    "\n",
    "# when True, excludes Alaska and Hawaii from EIA data sets\n",
    "contiguous_us_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters file\n",
    "parameters_file = 'GIM parameters file.xlsx'\n",
    "\n",
    "# folder for EIA data sets\n",
    "eia_path = '/Users/masoninman/Dropbox/GEM/LCA of natural gas use/EIA data for LCA of gas/'\n",
    "inputs_path = '/Users/masoninman/Dropbox/GEM/LCA of natural gas use/US cities LCA of gas model/US gas model inputs/'\n",
    "\n",
    "eia_data_2020_10_30_path = eia_path + 'EIA gas data released 2020-10-30 (production, consumption, trade)/'\n",
    "trade_flows_path = eia_data_2020_10_30_path\n",
    "gas_consump_data_path = eia_data_2020_10_30_path\n",
    "dry_file = 'EIA gas dry production NG_PROD_SUM_A_EPG0_FPD_mmcf_a released 2020-10-30.xls'\n",
    "    \n",
    "states_file = 'US states and abbreviations.xlsx'\n",
    "\n",
    "# Canada & Mexico data for dry gas production & gas production\n",
    "# https://www.eia.gov/international/data/country/CAN/natural-gas/dry-natural-gas-production\n",
    "# https://www.eia.gov/international/data/country/MEX/natural-gas/dry-natural-gas-production\n",
    "canada_international_file = 'EIA international - Canada - dry gas production and gas consumption 1980-2019 (INT-Export-10-11-2020).csv'\n",
    "mexico_international_file = 'EIA international - Mexico - dry gas production and gas consumption 1980-2019 (INT-Export-10-11-2020).csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(inputs_path + parameters_file, 'main parameters')\n",
    "ser = df.set_index('parameter name')['parameter value']\n",
    "parameters_main = ser\n",
    "\n",
    "data_year = int(parameters_main.at['data_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sources:\n",
    "* EIA US trade flows data: \"International & Interstate Movements of Natural Gas by State\" https://www.eia.gov/dnav/ng/ng_move_ist_a2dcu_nus_a.htm\n",
    "* EIA US dry gas production data: \"Natural Gas Gross Withdrawals and Production\" https://www.eia.gov/dnav/ng/ng_prod_sum_a_EPG0_FPD_mmcf_a.htm\n",
    "* EIA US gas consumption data: \"Natural Gas Consumption by End Use\" https://www.eia.gov/dnav/ng/ng_cons_sum_dcu_nus_a.htm\n",
    "* EIA international dry gas production data & gas consumption data: https://www.eia.gov/international/overview/world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create state list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_states_df_create_list(states_file):\n",
    "    states_df = pd.read_excel(inputs_path + states_file)\n",
    "    \n",
    "    if contiguous_us_only == True:\n",
    "        states_df = states_df.loc[~states_df['state'].isin(['Alaska', 'Hawaii'])]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    states_dict = states_df.set_index('state')['abbrev'].to_dict()\n",
    "    states_list = list(states_dict.keys())\n",
    "    \n",
    "    all_jurisdictions_list = states_list + ['Canada', 'Mexico', 'overseas']\n",
    "    \n",
    "    return(states_list, states_dict, all_jurisdictions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imported gas by state (or other jurisdiction)\n",
    "* NG trade by state from EIA's \"International & Interstate Movements of Natural Gas by State\"\n",
    "* https://www.eia.gov/dnav/ng/ng_move_ist_a2dcu_nus_a.htm\n",
    "* (choose individual states from dropdown to download each file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gas_trade_files_compile_and_clean(states_list, trade_flows_path):\n",
    "    \"\"\"\n",
    "    Read files for gross gas imports by each state.\n",
    "    \n",
    "    Do not need to read Federal GOM file because it has zero gross imports (only exports).\n",
    "    \n",
    "    Data for Federal GOM production will show up as imports by neighboring states, e.g., Louisiana.\n",
    "    \n",
    "    From EIA page: International & Interstate Movements of Natural Gas by State\n",
    "    https://www.eia.gov/dnav/ng/ng_move_ist_a2dcu_nus_a.htm\n",
    "    \n",
    "    \"\"\"\n",
    "    all_states_dfs = []\n",
    "\n",
    "    # sheets in EIA state imports files are:\n",
    "    # Data 1: Receipts (gross) [aka imports]\n",
    "    # Data 2: Deliveries (gross) [aka exports]\n",
    "    # Data 3: Net receipts [aka net imports]\n",
    "\n",
    "    for state in states_list:\n",
    "    #     print(f\"processing {state}\") # for db\n",
    "        state_abbrev = states_dict[state]\n",
    "        imports_file = f'NG_MOVE_IST_A2DCU_S{state_abbrev}_A.xls'\n",
    "\n",
    "        imports_state = pd.read_excel(\n",
    "            trade_flows_path + imports_file,\n",
    "            sheet_name='Data 1', # gross receipts (aka gross imports)\n",
    "            header=2\n",
    "        )\n",
    "\n",
    "        imports_state['year'] = imports_state['Date'].astype(str).str.split('-').str[0].astype(int)\n",
    "        imports_state = imports_state.loc[\n",
    "            (imports_state['year']>=2000) & (imports_state['year']<=data_year)]\n",
    "        imports_state = imports_state.set_index('year')\n",
    "        imports_state = imports_state.drop('Date', axis=1)\n",
    "\n",
    "        for col in imports_state.columns.tolist():\n",
    "            new_col = col\n",
    "\n",
    "            new_col = new_col.replace('Natural Gas ', '')\n",
    "            new_col = new_col.replace('Million Cubic Feet', 'MMcf')\n",
    "            new_col = new_col.replace(\"  \", \" \")\n",
    "            new_col = new_col.replace('Receipts from', 'Receipts From')\n",
    "            new_col = new_col.replace('Federal Offshore--Gulf of Mexico', 'Fed GOM')\n",
    "\n",
    "            new_col = new_col.split('(MMcf)')[0]\n",
    "            new_col = new_col.split('Natural Gas Imports + Intransit')[-1]\n",
    "            new_col = new_col.split('Receipts From')[-1]\n",
    "            new_col = new_col.strip()\n",
    "\n",
    "            imports_state = imports_state.rename(columns={col: new_col})\n",
    "\n",
    "        # to deal with error in Utah & Kansas due to duplicate columns\n",
    "        if state == 'Utah' or state=='Kansas':\n",
    "            cols = imports_state.columns.tolist()\n",
    "            new_cols = [x for x in cols if x != f'{state} Interstate Movements: Net Receipts of']\n",
    "            imports_state = imports_state[new_cols]\n",
    "\n",
    "        if state == 'Utah':\n",
    "            cols = imports_state.columns.tolist()\n",
    "            new_cols = [x for x in cols if x != f'{state} Interstate Movements: Receipts of']\n",
    "            imports_state = imports_state[new_cols]\n",
    "\n",
    "        # remove state at beginning of string\n",
    "        # (done this way to avoid removing 'Virginia' from 'West Virginia')\n",
    "        for col in imports_state.columns.tolist():\n",
    "            if col[0:len(state)] == state:\n",
    "                new_col = col.replace(f'{state} ', '')\n",
    "                imports_state = imports_state.rename(columns={col: new_col})\n",
    "\n",
    "        # remove 'Imports + Intransit From ' portion\n",
    "        for col in imports_state.columns.tolist():\n",
    "            if col[0:len('Imports + Intransit From ')] == 'Imports + Intransit From ':\n",
    "                new_col = col.replace('Imports + Intransit From ', '')\n",
    "                imports_state = imports_state.rename(columns={col: new_col})\n",
    "\n",
    "        # simplify subtotals\n",
    "        if 'Net International & Interstate Receipts' in imports_state.columns:\n",
    "            imports_state = imports_state.rename(columns={\n",
    "                'Net International & Interstate Receipts': 'International & Interstate'})\n",
    "        if 'Net Interstate Receipts' in imports_state.columns:\n",
    "            imports_state = imports_state.rename(columns={\n",
    "                'Net Interstate Receipts': 'Interstate'})\n",
    "        if 'Net International Receipts' in imports_state.columns:\n",
    "            imports_state = imports_state.rename(columns={\n",
    "                'Net International Receipts': 'International'})\n",
    "\n",
    "        # add column 'importing state'\n",
    "        imports_state['importing state'] = state\n",
    "\n",
    "        all_states_dfs += [imports_state]\n",
    "        \n",
    "        # TEST: LOOK FOR DUPLICATE COLUMNS\n",
    "        for state_df in all_states_dfs:\n",
    "            l = state_df.columns.tolist()\n",
    "            dups = list(set([x for x in l if l.count(x) > 1]))\n",
    "            if len(dups) > 0:\n",
    "                print(f\"for {state_df['importing state'].unique().tolist()[0]}, {dups}\")\n",
    "        # END OF TEST\n",
    "        \n",
    "    return all_states_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_state_dfs_into_one_df(all_states_dfs):\n",
    "    imports_all = pd.concat(all_states_dfs, sort=False)\n",
    "    imports_all = imports_all.reset_index()\n",
    "    imports_year = imports_all.loc[imports_all['year']==data_year].set_index('importing state').drop('year', axis=1)\n",
    "    imports_year.columns.name = 'exporting state'\n",
    "    imports_year.index.name = 'consuming state'\n",
    "    \n",
    "    return imports_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imports_from_overseas_and_remove_extraneous_columns(imports_year):\n",
    "    \"\"\"\n",
    "    Simplify to only keep only imports from Canada, Mexico, and overseas (which includes all other countries).\n",
    "    \n",
    "    Remove columns for various countries, and categories of imports. \n",
    "    \n",
    "    Also, in 2019 data, at least, there was a column 'Indinana'.\n",
    "    \n",
    "    Seems to be a typo for Indiana. Column had no data; if empty, remove; if not empty, raise warning.\n",
    "    \"\"\"\n",
    "    # calculate imports from overseas\n",
    "    imports_year['overseas'] = imports_year['International Receipts'] - imports_year[['Canada', 'Mexico']].sum(axis=1, skipna=True)\n",
    "\n",
    "    # check column 'Indinana':\n",
    "    indinana_ser = imports_year['Indinana'].dropna()\n",
    "    if len(indinana_ser) == 0:\n",
    "        imports_year = imports_year.drop('Indinana', axis=1)\n",
    "    else:\n",
    "        print('Error!' + ' There was a typo in a column name Indinana, and there was data in it:')\n",
    "        print(indinana_ser)\n",
    "    \n",
    "    # keep only columns in states_list + Fed GOM, Canada, Mexico, overseas\n",
    "    keep_cols = states_list + ['Fed GOM', 'Canada', 'Mexico', 'overseas']\n",
    "    imports_year = imports_year[keep_cols]\n",
    "    \n",
    "    return imports_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions\n",
    "(states_list, states_dict, all_jurisdictions_list) = read_states_df_create_list(states_file)\n",
    "all_states_dfs = read_gas_trade_files_compile_and_clean(states_list, trade_flows_path)\n",
    "imports_year = compile_state_dfs_into_one_df(all_states_dfs)\n",
    "imports_year = calculate_imports_from_overseas_and_remove_extraneous_columns(imports_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get overseas exports & attribute to production areas\n",
    "* To compare against EIA data, need to have complete attribution of all US gas production (or all contiguous US, at least)\n",
    "* Get total international gross exports of gas\n",
    "* Subtract gross exports to Canada & Mexico\n",
    "* Remainder will be gross overseas exports from each state\n",
    "* Do attribution back to production areas for overseas exports, same as for consumption by US states & DC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add Canada, Mexico, overseas\n",
    "* use US states' gross exports data, and select only exports to Canada or Mexico\n",
    "* for overseas, calculate exports other than to Canada and Mexico\n",
    "* sheets in EIA state imports files are:\n",
    "  * Data 1: Receipts (gross) [aka imports]\n",
    "  * Data 2: Deliveries (gross) [aka exports]\n",
    "  * Data 3: Net receipts [aka net imports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_exports_to_sel_country(sel_country, data_year):\n",
    "    \"\"\"\n",
    "    Get data on exports to selected country from each US state.\n",
    "    \n",
    "    From EIA data on gas trade, sheet 'Data 2' (deliveries, aka gross exports).\n",
    "    \"\"\"\n",
    "    \n",
    "    imports_by_sel_country_dfs = [] # initialize\n",
    "    \n",
    "    for state in states_list:\n",
    "    #     print(f\"processing {state}\")\n",
    "        state_abbrev = states_dict[state]\n",
    "\n",
    "        imports_file = f'NG_MOVE_IST_A2DCU_S{state_abbrev}_A.xls'\n",
    "        exports_state = pd.read_excel(\n",
    "            trade_flows_path + imports_file, \n",
    "            sheet_name='Data 2', # deliveries (aka gross exports)\n",
    "            header=2\n",
    "        )\n",
    "\n",
    "        exports_state['year'] = exports_state['Date'].astype(str).str.split('-').str[0].astype(int)\n",
    "        exports_state = exports_state.loc[\n",
    "            (exports_state['year']>=2000) & (exports_state['year']<=data_year)]\n",
    "        exports_state = exports_state.set_index('year')\n",
    "        exports_state = exports_state.drop('Date', axis=1)\n",
    "\n",
    "        for col in exports_state.columns.tolist():\n",
    "            new_col = col\n",
    "            \n",
    "            # to deal with anomaly & misspelling\n",
    "            new_col = new_col.replace('Lousiana Natural Gas Exports + Intransit to Portugal', 'Louisiana International Deliveries to')\n",
    "\n",
    "            new_col = new_col.replace('Natural Gas ', '')\n",
    "            new_col = new_col.replace('Million Cubic Feet', 'MMcf')\n",
    "            new_col = new_col.replace(\"  \", \" \")\n",
    "            new_col = new_col.replace('Federal Offshore--Gulf of Mexico', 'Fed GOM')\n",
    "\n",
    "            new_col = new_col.split('(MMcf)')[0]\n",
    "            new_col = new_col.split('Natural Gas Imports + Intransit')[-1]\n",
    "            new_col = new_col.split('Deliveries to')[-1]\n",
    "            new_col = new_col.strip()\n",
    "            \n",
    "            new_col = new_col.replace('International Deliveries to All Countries', 'All Countries')\n",
    "            new_col = new_col.replace('International Deliveries to ', '')\n",
    "            \n",
    "            new_col = new_col.replace('Interstate Deliveries to', '')\n",
    "        \n",
    "            exports_state = exports_state.rename(columns={col: new_col})\n",
    "            \n",
    "        dup_col = 'Exports + Intransit of'\n",
    "        if dup_col in exports_state.columns:\n",
    "            exports_state = exports_state.drop(f'{state} {dup_col}', axis=1)\n",
    "\n",
    "        # remove state at beginning of string\n",
    "        # (done this way to avoid removing 'Virginia' from 'West Virginia')\n",
    "        for col in exports_state.columns.tolist():\n",
    "            if col[0:len(state)] == state:\n",
    "                new_col = col.replace(f'{state} ', '')\n",
    "                exports_state = exports_state.rename(columns={col: new_col})\n",
    "                \n",
    "        # simplify subtotals\n",
    "        if f'International Deliveries to {sel_country}' in exports_state.columns:\n",
    "            exports_state = exports_state.rename(columns={\n",
    "                f'International Deliveries to {sel_country}': sel_country\n",
    "            })\n",
    "            \n",
    "        exports_state = exports_state.rename(columns={'International Deliveries': 'All Countries'})\n",
    "\n",
    "        # keep only deliveries to selected country:\n",
    "        try:\n",
    "            exports_to_sel_country = exports_state.copy()[[sel_country]]\n",
    "            \n",
    "            # add column 'exporting juris'\n",
    "            exports_to_sel_country['exporting juris'] = state\n",
    "            imports_by_sel_country_dfs += [exports_to_sel_country]\n",
    "        \n",
    "        except:\n",
    "            # no deliveries to sel_country from this state\n",
    "            # print(f\"for {state}, no deliveries to {sel_country}?\")\n",
    "            pass       \n",
    "        \n",
    "    # after looping through all states\n",
    "    imports = pd.concat(imports_by_sel_country_dfs, sort=False)\n",
    "    imports = imports.reset_index()\n",
    "    \n",
    "    imports_year = imports.loc[imports['year']==data_year].set_index('exporting juris').drop('year', axis=1)\n",
    "    imports_year = imports_year.loc[imports_year[sel_country] > 5] # exclude negligible quantities\n",
    "    \n",
    "    imports_year.columns.name = 'importing juris'\n",
    "        \n",
    "    return imports_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overseas_exports_from_us(\n",
    "    imports_all_countries_year, \n",
    "    imports_canada_year, \n",
    "    imports_mexico_year):\n",
    "    \"\"\"\n",
    "    Like for Canada and Mexico, calculate all US exports that go overseas.\n",
    "    \n",
    "    This is to be able to calculate all attributions back to US producing states, \n",
    "    and compare against EIA data for production.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.concat([\n",
    "        imports_all_countries_year['All Countries'],\n",
    "        -1*imports_canada_year['Canada'],\n",
    "        -1*imports_mexico_year['Mexico']\n",
    "    ], axis=1, sort=False)\n",
    "\n",
    "    df['overseas'] = df.sum(axis=1)\n",
    "    imports_overseas_year = df[['overseas']]\n",
    "    \n",
    "    return imports_overseas_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function for Canada & Mexico\n",
    "imports_canada_year = read_exports_to_sel_country('Canada', data_year)\n",
    "imports_mexico_year = read_exports_to_sel_country('Mexico', data_year)\n",
    "\n",
    "# run function for all international deliveries\n",
    "# columns are, e.g., 'Louisiana Natural Gas International Deliveries to All Countries (Million Cubic Feet)'\n",
    "imports_all_countries_year = read_exports_to_sel_country('All Countries', data_year)\n",
    "\n",
    "# calculate overseas \n",
    "imports_overseas_year = calculate_overseas_exports_from_us(\n",
    "    imports_all_countries_year, \n",
    "    imports_canada_year, \n",
    "    imports_mexico_year)\n",
    "\n",
    "# add Canada, Mexico, and overseas imports to df imports_year\n",
    "# they will then be in the df as \"consuming states\"\n",
    "imports_year = imports_year.append(imports_canada_year.T, sort=False)\n",
    "imports_year = imports_year.append(imports_mexico_year.T, sort=False)\n",
    "imports_year = imports_year.append(imports_overseas_year.T, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "# imports_year.to_csv(\n",
    "#     inputs_path + \n",
    "#     f'GIM output - EIA gross imports by consuming state and immediate importer for {data_year}.csv', \n",
    "#     index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gas production (dry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dry_gas_production_data(dry_file):\n",
    "    # data for production for each state\n",
    "    # use 'dry production'\n",
    "    # units are MMcf\n",
    "\n",
    "    dry_1 = pd.read_excel(\n",
    "        eia_data_2020_10_30_path + dry_file, \n",
    "        header=2,\n",
    "        sheet_name='Data 1'\n",
    "    )\n",
    "\n",
    "    dry_2 = pd.read_excel(\n",
    "        eia_data_2020_10_30_path + dry_file, \n",
    "        header=2,\n",
    "        sheet_name='Data 2'\n",
    "    )\n",
    "\n",
    "    dry = pd.merge(dry_1, dry_2, left_on='Date', right_on='Date')\n",
    "\n",
    "    dry['year'] = dry['Date'].astype(str).str.split('-').str[0].astype(int)\n",
    "\n",
    "    # only keep years from 2000 onward\n",
    "    # prior to 1997 used different accounting, and there was a discrepancy in 1999\n",
    "    # drop latest year (2019) because it doesn't include breakdown by state for 'Other States' (sheet 'Data 2')\n",
    "    dry = dry.loc[(dry['year']>=2000) & (dry['year']<=data_year)]\n",
    "\n",
    "    dry = dry.set_index('year')\n",
    "    dry = dry.drop('Date', axis=1)\n",
    "    \n",
    "    return dry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dry_prod_data_select_data_year(dry):\n",
    "    for col in dry.columns:\n",
    "        if ' Dry Natural Gas Production (MMcf)' in col:\n",
    "            new_col = col.replace(' Dry Natural Gas Production (MMcf)', '')\n",
    "            dry = dry.rename(columns={col: new_col})\n",
    "        elif ' Dry Natural Gas Production (Million Cubic Feet)' in col:\n",
    "            new_col = col.replace(' Dry Natural Gas Production (Million Cubic Feet)', '')\n",
    "            dry = dry.rename(columns={col: new_col})\n",
    "        elif ' Dry Natural Gas Production  (Million Cubic Feet)' in col:\n",
    "            new_col = col.replace(' Dry Natural Gas Production  (Million Cubic Feet)', '')\n",
    "            dry = dry.rename(columns={col: new_col})\n",
    "        elif ' Natural Gas Dry Production (Million Cubic Feet)' in col:\n",
    "            new_col = col.replace(' Natural Gas Dry Production (Million Cubic Feet)', '')\n",
    "            dry = dry.rename(columns={col: new_col})\n",
    "        elif ' Dry Production of Natural Gas (Million Cubic Feet)' in col:\n",
    "            new_col = col.replace(' Dry Production of Natural Gas (Million Cubic Feet)', '')\n",
    "            dry = dry.rename(columns={col: new_col})\n",
    "        else:\n",
    "            print(col)\n",
    "            \n",
    "    # drop extraneous columns for California\n",
    "    # unlike others, Federal Offshore California included in California total\n",
    "    for col in [\n",
    "        'Calif--Onshore',\n",
    "        'California--State Offshore', \n",
    "        # 'Federal Offshore California',\n",
    "    ]:\n",
    "        try:\n",
    "            dry = dry.drop(col, axis=1)\n",
    "        except:\n",
    "            print(f\"col wasn't in df: {col}\")\n",
    "\n",
    "    # drop extraneous columns for offshore\n",
    "    for state in ['Alabama', 'Alaska', 'Louisiana', 'Texas']:\n",
    "        try:\n",
    "            dry = dry.drop(f'{state}--Onshore', axis=1)\n",
    "        except:\n",
    "            print(f\"for Onshore, state wasn't in df: {state}\")\n",
    "        try:\n",
    "            dry = dry.drop(f'{state}--State Offshore', axis=1)\n",
    "        except:\n",
    "            print(f\"for Offshore, state wasn't in df: {state}\")\n",
    "\n",
    "    # drop federal offshore attributed to states\n",
    "    # (was only used 1992-1998)\n",
    "    for col in [\n",
    "        'Federal Offshore--Alabama',\n",
    "        'Federal Offshore--Louisiana', \n",
    "        'Federal Offshore--Texas',\n",
    "        ]:\n",
    "        try:\n",
    "            dry = dry.drop(col, axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    dry = dry.drop(['U.S.', 'Other States'], axis=1)\n",
    "    dry = dry.rename(columns={'Federal Offshore--Gulf of Mexico': 'Fed GOM'})\n",
    "    \n",
    "    # select only the year of analysis\n",
    "    dry_year = dry.copy().loc[data_year]\n",
    "\n",
    "    return dry_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_international_dry_gas_production_and_consump(international_file, country):\n",
    "    \"\"\"\n",
    "    In argument, units are Bcf. Converts to MMcf for output.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        eia_path + international_file, \n",
    "        header=1)\n",
    "    df = df.rename(columns={'Unnamed: 1': 'Category'})\n",
    "    df = df.drop('API', axis=1)\n",
    "    df['Category'] = df['Category'].str.strip()\n",
    "\n",
    "    prod_row = df.loc[df['Category']=='Production'].index[0]\n",
    "    dry_prod_row = prod_row + 2\n",
    "\n",
    "    # check results\n",
    "    if df.loc[dry_prod_row, 'Category'] == 'Dry natural gas (bcf)':\n",
    "        dry_prod_year_bcf = df.at[dry_prod_row, str(data_year)]\n",
    "    else:\n",
    "        print(\"Error!\" + f\" Category result was unexpected; was {df.loc[dry_prod_row, 'Category']}\")\n",
    "\n",
    "    consump_row = df.loc[df['Category']=='Consumption'].index[0]\n",
    "    dry_consump_row = consump_row + 1\n",
    "    if df.loc[dry_consump_row, 'Category'] == 'Dry natural gas (bcf)':\n",
    "        consump_year_bcf = df.at[dry_consump_row, str(data_year)]\n",
    "    else:\n",
    "        print(\"Error!\" + f\" Category result was unexpected; was {df.loc[dry_prod_row, 'Category']}\")\n",
    "\n",
    "    mmcf_per_bcf = 1000\n",
    "    dry_prod_year_mmcf = dry_prod_year_bcf * mmcf_per_bcf\n",
    "    consump_year_mmcf = consump_year_bcf * mmcf_per_bcf\n",
    "    \n",
    "    return(dry_prod_year_mmcf, consump_year_mmcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_canada_mexico_dry_production_and_consump(\n",
    "    dry_year, canada_dry_prod_year_mmcf, mexico_dry_prod_year_mmcf):\n",
    "\n",
    "    # put production data into df dry_year\n",
    "    dry_year.at['Canada'] = canada_dry_prod_year_mmcf\n",
    "    dry_year.at['Mexico'] = mexico_dry_prod_year_mmcf\n",
    "    \n",
    "    return dry_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions\n",
    "dry = read_dry_gas_production_data(dry_file)\n",
    "dry_year = clean_dry_prod_data_select_data_year(dry)\n",
    "\n",
    "(canada_dry_prod_year_mmcf, canada_consump_year_mmcf) = read_international_dry_gas_production_and_consump(\n",
    "    canada_international_file, 'Canada')\n",
    "(mexico_dry_prod_year_mmcf, mexico_consump_year_mmcf) = read_international_dry_gas_production_and_consump(\n",
    "    mexico_international_file, 'Mexico')\n",
    "dry_year = add_canada_mexico_dry_production_and_consump(\n",
    "    dry_year, canada_dry_prod_year_mmcf, mexico_dry_prod_year_mmcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gross supply (dry production + imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gross_supply_by_state(imports_year, dry_year):\n",
    "    \"\"\"\n",
    "    Calculate gross supply for each state (and DC, Canada, and Mexico). \n",
    "    Gross supply = dry production + gross imports by each state (from other states or jurisdictions)\n",
    "\n",
    "    Use dry production because that's \"consumer-grade\" gas,\n",
    "    which goes into transmission pipelines and is sent across state lines;   \n",
    "    EIA pipeline flows between states should most closely match dry gas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set threshold for import data to ignore\n",
    "    # applying the threshold removes NaNs & tiny values that are neglibile and possibly erroneous\n",
    "    supply_threshold = 500 # units MMcf\n",
    "\n",
    "    gross_supply_list = []\n",
    "\n",
    "    # iterate through all states to calculate gross supply for each\n",
    "    for juris in all_jurisdictions_list:\n",
    "    #     print(f\"processing {juris}\") # for db\n",
    "\n",
    "        # create series for imports for each juris\n",
    "        try:\n",
    "            imports_juris = imports_year.loc[juris].T\n",
    "        except:\n",
    "            imports_juris = pd.Series()\n",
    "\n",
    "        # filter to keep only positive values above supply_threshold\n",
    "        imports_juris = imports_juris.loc[imports_juris > supply_threshold]\n",
    "\n",
    "        for category in ['Interstate', 'International', 'International & Interstate']:\n",
    "            try:\n",
    "                imports_juris = imports_juris.drop(category)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        gross_supply_juris = imports_juris.copy()\n",
    "        try:\n",
    "            dry_juris = dry_year.at[juris]\n",
    "        except:\n",
    "            dry_juris = 0\n",
    "        gross_supply_juris.at[juris] = dry_juris\n",
    "\n",
    "        gross_supply_juris.name = juris\n",
    "        gross_supply_juris = pd.DataFrame(gross_supply_juris).T\n",
    "        gross_supply_juris.index.name = 'importing juris'\n",
    "        gross_supply_juris.columns.name = 'from juris'\n",
    "\n",
    "        gross_supply_list += [gross_supply_juris]\n",
    "\n",
    "    gross_supply = pd.concat(gross_supply_list, sort=False)\n",
    "    gross_supply.columns.name = 'from juris'\n",
    "    \n",
    "    # drop subtotals:\n",
    "    for col in [\n",
    "        'International & Interstate Receipts', \n",
    "        'International Receipts',\n",
    "        'Interstate Receipts', \n",
    "        'All Countries', \n",
    "        'All States'\n",
    "    ]:\n",
    "        try:\n",
    "            gross_supply = gross_supply.drop(col, axis=1)\n",
    "        except:\n",
    "            # these were already removed earlier; don't need to print line below\n",
    "            # print(f\"gross_supply didn't have col {col}\")\n",
    "            pass\n",
    "    \n",
    "    return gross_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_supply_to_production_using_recursive_calculation(gross_supply, all_jurisdictions_list):\n",
    "    \"\"\"\n",
    "    Create df for first-level supplies for a juris, as fractions.\n",
    "    Each supply is labeled as import or production.\n",
    "    For imports from each juris, trace back to origins of that juris's gross supply.\n",
    "    Keep running while there are any imports listed in the df.\n",
    "    \n",
    "    Exports results to csv files for each state, since the results are very long.\n",
    "    These files are then recompiled to calculate results.\n",
    "    \"\"\"\n",
    "\n",
    "    supp_fract_prod_sums_list = [] # initialize\n",
    "    \n",
    "    # show threshold fraction for attributing consumption back to sources (e.g., 99%)\n",
    "    # threshold_fract set in parameters at start of notebook\n",
    "    print(f\"threshold is {round(threshold_fract*100, 2)}%\")\n",
    "\n",
    "    # set timestamp once, for all states to be processed, so all saved files will have the same timestamp\n",
    "    supp_fract_prod_timestamp = time.strftime('%Y-%m-%d_%H%M', time.localtime())\n",
    "\n",
    "    for consump_juris in all_jurisdictions_list:\n",
    "        # from gross supply df, calculate fractions of supply \n",
    "        # from each neighboring state (or own state production)\n",
    "        supp_fract = create_supp_fract(gross_supply, consump_juris)\n",
    "        \n",
    "        # create df for state's own production\n",
    "        supp_fract_prod_own = create_df_of_consuming_juris_own_production(supp_fract, consump_juris)\n",
    "\n",
    "        # create df for recording production from all states\n",
    "        # (starts with consuming state's own production)\n",
    "        supp_fract_prod = supp_fract_prod_own.copy()\n",
    "\n",
    "        # from supply fractions, create df of only imports;\n",
    "        # this is an initial df that is added to iteratively below\n",
    "        supp_fract_imp_init = supp_fract.copy().loc[supp_fract['type']=='import']\n",
    "        supp_fract_imp_init['path'] = f'{consump_juris}-' + supp_fract_imp_init.index\n",
    "        supp_fract_imp_init = supp_fract_imp_init.reset_index(drop=True)\n",
    "        \n",
    "        imports_init_shares = create_imports_init_shares(supp_fract_imp_init)\n",
    "\n",
    "        # create list of jurisdictions to iterate through\n",
    "        supp_fract_imp_init_juris = supp_fract_imp_init['path'].str.split('-').str[-1].tolist()\n",
    "        \n",
    "        for init_import_juris in supp_fract_imp_init_juris:\n",
    "            # iterate through supplies to track origins of gas\n",
    "            (supp_fract_prod_1juris, iter_num) = create_supp_fract_prod_1juris(\n",
    "                init_import_juris,\n",
    "                supp_fract_imp_init,\n",
    "                imports_init_shares,\n",
    "                consump_juris\n",
    "            )\n",
    "            \n",
    "            # run test to check that scaled values total 100%\n",
    "            test_scaled_values_sum_to_100_percent(supp_fract_prod_1juris, imports_init_shares, init_import_juris)\n",
    "\n",
    "            # record number of iterations reached\n",
    "            supp_fract_prod_1juris = record_number_of_iterations_reached(iter_num, supp_fract_prod_1juris)\n",
    "\n",
    "            # put the final df supp_fract_prod_1juris into supp_fract_prod\n",
    "            supp_fract_prod = supp_fract_prod.append(supp_fract_prod_1juris, sort=False)\n",
    "            supp_fract_prod = supp_fract_prod.drop('share raw', axis=1)\n",
    "\n",
    "        # run test\n",
    "        test_consuming_state_total_supply_sums_to_100_percent(supp_fract_prod)\n",
    "\n",
    "        # export supp_fract_prod\n",
    "        supp_fract_prod.to_csv(\n",
    "            inputs_path + \n",
    "            f'US gas model - {consump_juris} results supp_fract_prod {max_iter} iterations ({supp_fract_prod_timestamp}).csv', \n",
    "            index=False)\n",
    "\n",
    "        # collect all dfs into a list of dfs\n",
    "        supp_fract_prod_sums_list = collect_contributions_from_each_producing_state(\n",
    "            supp_fract_prod, consump_juris, supp_fract_prod_sums_list)\n",
    "\n",
    "    supp_fract_prod_sums_all = sum_all_state_dfs(supp_fract_prod_sums_list)\n",
    "    \n",
    "    # export\n",
    "    supp_fract_prod_sums_all.to_excel(\n",
    "        f'GIM gas flows supp_fract_prod_sums_all from gross trade for {data_year}, {max_iter} iterations ({supp_fract_prod_timestamp}).xlsx')\n",
    "\n",
    "    return(supp_fract_prod_sums_all, supp_fract_prod_timestamp)\n",
    "# end of attribute_supply_to_production_using_recursive_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supp_fract(gross_supply, consump_juris):\n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation\n",
    "    \"\"\"\n",
    "    supp_fract_ser = gross_supply.loc[consump_juris].dropna() / gross_supply.loc[consump_juris].sum()\n",
    "    # set name to be 'share raw'; overwrites consump_juris (e.g., 'California')\n",
    "    supp_fract_ser.name = 'share raw'\n",
    "\n",
    "    supp_fract = pd.DataFrame(supp_fract_ser)\n",
    "    for row in supp_fract.index:\n",
    "        if row == consump_juris:\n",
    "            supp_fract.at[row, 'type'] = 'production'\n",
    "        else:\n",
    "            supp_fract.at[row, 'type'] = 'import'\n",
    "\n",
    "    return supp_fract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_of_consuming_juris_own_production(supp_fract, consump_juris):\n",
    "    # start df for production with the consuming juris's own production\n",
    "    supp_fract_prod_own = supp_fract.copy().loc[supp_fract['type']=='production']\n",
    "    supp_fract_prod_own = supp_fract_prod_own.reset_index(drop=True)\n",
    "    # for state's own production, no scaling required\n",
    "    supp_fract_prod_own['share scaled'] = supp_fract_prod_own['share raw']\n",
    "    supp_fract_prod_own['path'] = consump_juris\n",
    "    supp_fract_prod_own['number of iterations'] = int(0)\n",
    "\n",
    "    return supp_fract_prod_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imports_init_shares(supp_fract_imp_init):\n",
    "    # create df imports_init_shares for saving values for import shares from each immediate importer\n",
    "    df = supp_fract_imp_init.copy()\n",
    "    df['path end'] = df['path'].str.split('-').str[-1]\n",
    "    df = df.set_index('path end')\n",
    "    imports_init_shares = df['share raw']\n",
    "    \n",
    "    return imports_init_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_number_of_iterations_reached(iter_num, supp_fract_prod_1juris):\n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation.\n",
    "    \n",
    "    Puts number of iterations reached into dataframe.\n",
    "    \"\"\"\n",
    "    max_iter_num = iter_num + 1\n",
    "    try:\n",
    "        max_iter_num = int(max_iter_num)\n",
    "    except:\n",
    "        print(f\"couldn't convert max_iter_num to int: {max_iter_num}\")\n",
    "        pass\n",
    "    supp_fract_prod_1juris['number of iterations'] = max_iter_num\n",
    "\n",
    "    return supp_fract_prod_1juris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scaled_values_sum_to_100_percent(supp_fract_prod_1juris, imports_init_shares, init_import_juris):\n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation\n",
    "    \"\"\"\n",
    "    # TEST: add test to check that scaled values add up to 100%\n",
    "    scaled_sum = supp_fract_prod_1juris['share scaled'].sum()\n",
    "    import_share = imports_init_shares.loc[init_import_juris]\n",
    "    if abs(scaled_sum - import_share) < 1e-6:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Error!\" + f\" abs(1 - scaled_sum/import_share): {abs(1 - scaled_sum/import_share)}\")\n",
    "    # END OF TEST\n",
    "    # no return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_consuming_state_total_supply_sums_to_100_percent(supp_fract_prod):\n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation\n",
    "    \"\"\"\n",
    "    # TEST: check that for each consuming state, the supply shares add up to 1 (100%)\n",
    "    share_sum = supp_fract_prod['share scaled'].sum()\n",
    "    if abs(1 - share_sum) < 1e-6:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Error!\" + f\" The sum of shares wasn't 100% (within rounding error); it was: {share_sum}\")\n",
    "    # END OF TEST\n",
    "    # no return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supp_fract_prod_1juris(\n",
    "    init_import_juris,\n",
    "    supp_fract_imp_init,\n",
    "    imports_init_shares,\n",
    "    consump_juris\n",
    "    ):  \n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation\n",
    "    \"\"\"\n",
    "    \n",
    "    # select imports for the specified initial import juris\n",
    "    supp_fract_imp_1juris = supp_fract_imp_init.loc[\n",
    "        supp_fract_imp_init['path'].str.split('-').str[-1]==init_import_juris\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # initialize df for production data\n",
    "    supp_fract_prod_1juris = pd.DataFrame()\n",
    "\n",
    "    # iteratively calculate sources of supply for initial import juris\n",
    "    for iter_num in range(0, max_iter):\n",
    "        if len(supp_fract_prod_1juris)==0:\n",
    "            supp_fract_prod_1juris_sum = 0 # initialize\n",
    "        elif len(supp_fract_prod_1juris)>0:\n",
    "            supp_fract_prod_1juris_sum = supp_fract_prod_1juris['share raw'].sum()\n",
    "        else:\n",
    "            print('Error!' + f\" Unexpected value for len(supp_fract_prod_1juris): {len(supp_fract_prod_1juris)}\")\n",
    "\n",
    "        if supp_fract_prod_1juris_sum / imports_init_shares.loc[init_import_juris] < threshold_fract:\n",
    "            if len(supp_fract_imp_1juris) > 0:\n",
    "                # get data for first row of imports from df\n",
    "                sel_row = 0\n",
    "                source_juris = supp_fract_imp_1juris.loc[sel_row, 'path'].split('-')[-1]\n",
    "                source_juris_share = supp_fract_imp_1juris.loc[sel_row, 'share raw']\n",
    "                source_juris_path = supp_fract_imp_1juris.loc[sel_row, 'path'] # modified later\n",
    "\n",
    "                # remove source_juris data from supp_fract_imp_1juris\n",
    "                supp_fract_imp_1juris = supp_fract_imp_1juris.loc[supp_fract_imp_1juris.index!=sel_row]\n",
    "\n",
    "                # run sub-function\n",
    "                (supp_fract_prod_1juris, supp_fract_imp_1juris) = create_prod_and_imports_for_each_juris(\n",
    "                    source_juris, \n",
    "                    source_juris_share, \n",
    "                    source_juris_path, \n",
    "                    supp_fract_prod_1juris, \n",
    "                    supp_fract_imp_1juris, \n",
    "                    gross_supply, \n",
    "                    all_jurisdictions_list)\n",
    "\n",
    "            else:\n",
    "                # len(supp_fract_imp_1juris)==0\n",
    "                # no imports to process\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            # supp_fract_prod['share raw'].sum() >= threshold_fract\n",
    "            # assigned imports have reached the threshold; stop iterations                \n",
    "            break\n",
    "            \n",
    "    # end of: for iter_num in range(0, max_iter)\n",
    "    \n",
    "    ratio_covered = supp_fract_prod_1juris['share raw'].sum() / imports_init_shares.loc[init_import_juris]       \n",
    "    supp_fract_prod_1juris['share scaled'] = supp_fract_prod_1juris['share raw'] / ratio_covered\n",
    "    print(f\"for {consump_juris} via {init_import_juris}, completed {iter_num+1} iterations, covered {(ratio_covered*100).round(2)}%\")\n",
    "\n",
    "    return(supp_fract_prod_1juris, iter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prod_and_imports_for_each_juris(\n",
    "    source_juris, \n",
    "    source_juris_share, \n",
    "    source_juris_path, \n",
    "    supp_fract_prod_1juris, \n",
    "    supp_fract_imp_1juris, \n",
    "    gross_supply, \n",
    "    all_jurisdictions_list):\n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation\n",
    "    \"\"\"\n",
    "    if source_juris in all_jurisdictions_list:\n",
    "        # get data from gross_supply for source state\n",
    "        source_juris_sources = gross_supply.loc[source_juris].dropna()\n",
    "        source_juris_total = gross_supply.loc[source_juris].sum()\n",
    "        sources_fract = source_juris_sources / source_juris_total\n",
    "\n",
    "        # multiply by share of imports (for given source juris)\n",
    "        sources_scaled = sources_fract * source_juris_share\n",
    "\n",
    "        # assign production or import label\n",
    "        sources_scaled = pd.DataFrame(sources_scaled)\n",
    "        for row in sources_scaled.index:\n",
    "            if row == source_juris:\n",
    "                sources_scaled.loc[row, 'type'] = 'production'\n",
    "            else:\n",
    "                sources_scaled.loc[row, 'type'] = 'import'\n",
    "\n",
    "        # rename column from source_juris to 'share raw'\n",
    "        # (for appending to dfs below)\n",
    "        sources_scaled = sources_scaled.rename(columns={source_juris: 'share raw'})\n",
    "\n",
    "        sources_scaled['path'] = f'{source_juris_path}-' + sources_scaled.index\n",
    "\n",
    "        # if production, put value into supp_fract_prod_1juris\n",
    "        sources_scaled_prod = sources_scaled.loc[sources_scaled['type']=='production']\n",
    "        sources_scaled_prod = sources_scaled_prod.reset_index(drop=True)                        \n",
    "        supp_fract_prod_1juris = supp_fract_prod_1juris.append(sources_scaled_prod, sort=False)\n",
    "\n",
    "        # if import, put values into supp_fract_imp_1juris\n",
    "        sources_scaled_imp = sources_scaled.loc[sources_scaled['type']=='import']\n",
    "        supp_fract_imp_1juris = supp_fract_imp_1juris.append(sources_scaled_imp, sort=False)\n",
    "\n",
    "    else:\n",
    "        # source_juris is not in all_jurisdictions_list; \n",
    "        # source_juris is Fed GOM or foreign country (other than Canada or Mexico)\n",
    "        supp_fract_prod_1juris = supp_fract_prod_1juris.reset_index(drop=True)\n",
    "        next_index = supp_fract_prod_1juris.index.max() + 1\n",
    "\n",
    "        supp_fract_prod_1juris.loc[next_index, 'share raw'] = source_juris_share\n",
    "        supp_fract_prod_1juris.loc[next_index, 'type'] = 'production'\n",
    "        supp_fract_prod_1juris.loc[next_index, 'path'] = source_juris_path + f'-{source_juris}'\n",
    "\n",
    "    # reset index so there is always a 0th index for the next iteration\n",
    "    supp_fract_imp_1juris = supp_fract_imp_1juris.reset_index(drop=True)\n",
    "\n",
    "    return(supp_fract_prod_1juris, supp_fract_imp_1juris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_contributions_from_each_producing_state(\n",
    "    supp_fract_prod, consump_juris, supp_fract_prod_sums_list):\n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation\n",
    "    \"\"\"\n",
    "    # sum contributions from each producing state\n",
    "    df = supp_fract_prod.copy()\n",
    "    df['path_end'] = df['path'].str.split('-').str[-1]\n",
    "    # for sums, change column name 'share scaled' to consump_juris\n",
    "    df = df.rename(columns={'share scaled': consump_juris})\n",
    "    df = df.groupby('path_end')[consump_juris].sum().sort_values(ascending=False)\n",
    "    df = pd.DataFrame(df, columns=[consump_juris]).T\n",
    "    df.index.name = 'consump juris'\n",
    "    df.columns.name = 'prod juris'    \n",
    "    supp_fract_prod_sums_list += [df]\n",
    "\n",
    "    return supp_fract_prod_sums_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_all_state_dfs(supp_fract_prod_sums_list):\n",
    "    \"\"\"\n",
    "    Sub-function of attribute_supply_to_production_using_recursive_calculation\n",
    "    \"\"\"\n",
    "    # after for loop for all states, sum all dfs in list\n",
    "    supp_fract_prod_sums_all = pd.concat(supp_fract_prod_sums_list, sort=False)\n",
    "    supp_fract_prod_sums_all.columns.name = 'producing juris'\n",
    "    supp_fract_prod_sums_all.index.name = 'consuming juris'\n",
    "    supp_fract_prod_sums_all = supp_fract_prod_sums_all.sort_index(axis=0)\n",
    "    supp_fract_prod_sums_all = supp_fract_prod_sums_all.sort_index(axis=1)\n",
    "    \n",
    "    return supp_fract_prod_sums_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_supply = calc_gross_supply_by_state(imports_year, dry_year)\n",
    "(supp_fract_prod_sums_all, \n",
    " supp_fract_prod_timestamp) = attribute_supply_to_production_using_recursive_calculation(\n",
    "    gross_supply, all_jurisdictions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST: check implied production by state\n",
    "* Given the shares of each state's consumption that are supposed to come from each state's production...\n",
    "* Can calculate what total production level is implied\n",
    "* Multiply each state's consumption shares by its total consumption\n",
    "* Then sum production from each producing state\n",
    "* EIA data on consumption doesn't exactly match any of the production categories (e.g., dry), so things won't exactly match up\n",
    "* But if within ~5%, probably good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can import results from prior run\n",
    "# supp_fract_prod_sums_all = pd.read_excel(\n",
    "#     '/Users/masoninman/Dropbox/GEM/LCA of natural gas use/' + \n",
    "#     'US gas trade model supp_fract_prod_sums_all from gross trade 5000 iterations (2020-08-08_1039).xlsx'\n",
    "# )\n",
    "# supp_fract_prod_sums_all = supp_fract_prod_sums_all.set_index('consuming juris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supp_dry_comparison(supp_fract_prod_sums_all, dry_year):\n",
    "    \n",
    "    # run sub-functions\n",
    "    consump_juris_tot_all = read_eia_consump_data(canada_consump_year_mmcf, mexico_consump_year_mmcf)\n",
    "    supp_quant_year_sums = calc_supp_quant_sums(supp_fract_prod_sums_all, consump_juris_tot_all)\n",
    "    \n",
    "    supp_dry_comparison = pd.concat([supp_quant_year_sums, dry_year], axis=1, sort=False)\n",
    "    supp_dry_comparison = supp_dry_comparison.rename(columns={0: 'from consump', data_year: 'dry production'})\n",
    "\n",
    "    # replace very small values with NaN\n",
    "    for state in supp_dry_comparison.index:\n",
    "        if supp_dry_comparison.at[state, 'from consump'] < 1:\n",
    "            supp_dry_comparison.at[state, 'from consump'] = np.NaN\n",
    "\n",
    "    supp_dry_comparison['fract diff'] = (supp_dry_comparison['from consump']-supp_dry_comparison['dry production'])/supp_dry_comparison['dry production']\n",
    "    supp_dry_comparison = supp_dry_comparison.sort_values(by=['dry production'], ascending=False)\n",
    "\n",
    "    # Note: would expect attributed volumes to be somewhat higher than dry production\n",
    "    # because consumer-grade gas includes some ethane, which is excluded from dry production data\n",
    "    \n",
    "    return supp_dry_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_eia_consump_data(canada_consump_year_mmcf, mexico_consump_year_mmcf):\n",
    "    \"\"\"\n",
    "    Sub-function of create_supp_dry_comparison\n",
    "    \n",
    "    Read data on consumption by state.\n",
    "    \n",
    "    From EIA page: Natural Gas Consumption by End Use\n",
    "    https://www.eia.gov/dnav/ng/ng_cons_sum_dcu_STX_a.htm    \n",
    "    \"\"\"\n",
    "    # import EIA data on gas consumption by state\n",
    "    consump_juris_tot_all = pd.DataFrame() # initialize\n",
    "\n",
    "    for state in states_list:\n",
    "        state_abbrev = states_dict[state]\n",
    "        consump_juris_df = pd.read_excel(\n",
    "            gas_consump_data_path +\n",
    "            f'NG_CONS_SUM_DCU_S{state_abbrev}_A.xls', \n",
    "            sheet_name='Data 1', \n",
    "            header=2,\n",
    "        )\n",
    "\n",
    "        consump_juris_df['year'] = consump_juris_df['Date'].astype(str).str.split('-').str[0].astype(int)\n",
    "\n",
    "        consump_juris_df = consump_juris_df.set_index('year')\n",
    "        consump_juris_df = consump_juris_df.drop('Date', axis=1)\n",
    "\n",
    "        # keep only data_year\n",
    "        consump_juris_df = consump_juris_df.loc[consump_juris_df.index==data_year]\n",
    "\n",
    "        # pull out total consumption series\n",
    "        consump_juris_df_tot = consump_juris_df[[f'{state} Natural Gas Total Consumption (MMcf)']]\n",
    "        consump_juris_df_tot = consump_juris_df_tot.rename(columns={f'{state} Natural Gas Total Consumption (MMcf)': state})\n",
    "\n",
    "        consump_juris_tot_all = pd.concat([consump_juris_tot_all, consump_juris_df_tot], axis=1)\n",
    "\n",
    "    # EIA international data on consumption from\n",
    "    if data_year > 2019:\n",
    "        print(\"Error!\" + f\" Not set up to add consumption data for Canada and Mexico for data_year {data_year}.\")\n",
    "    else:\n",
    "        consump_juris_tot_all.at[data_year, 'Canada'] = canada_consump_year_mmcf\n",
    "        consump_juris_tot_all.at[data_year, 'Mexico'] = mexico_consump_year_mmcf\n",
    "\n",
    "    # for rest of world, treat as though only consumption is from US exports\n",
    "    # here we only need to attribute US exports back to producing US states, \n",
    "    consump_juris_tot_all.at[data_year, 'overseas'] = imports_overseas_year['overseas'].sum()\n",
    "    \n",
    "    return consump_juris_tot_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_supp_quant_sums(supp_fract_prod_sums_all, consump_juris_tot_all):\n",
    "    \"\"\"Sub-function of create_supp_dry_comparison\"\"\"\n",
    "    supp_quant_year = pd.DataFrame()\n",
    "\n",
    "    for consump_juris in supp_fract_prod_sums_all.index:\n",
    "        consump_juris_year_quant = consump_juris_tot_all.loc[data_year, consump_juris]\n",
    "        supp_quant_state_year = consump_juris_year_quant * supp_fract_prod_sums_all.loc[supp_fract_prod_sums_all.index==consump_juris]\n",
    "\n",
    "        supp_quant_year = supp_quant_year.append(supp_quant_state_year)\n",
    "\n",
    "    supp_quant_year_sums = supp_quant_year.sum(axis=0)\n",
    "    \n",
    "    return supp_quant_year_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_dry_comparison = create_supp_dry_comparison(supp_fract_prod_sums_all, dry_year)\n",
    "\n",
    "# show:\n",
    "supp_dry_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_us_only = supp_dry_comparison.drop(['Canada', 'Alaska'])[['from consump', 'dry production']].sum(axis=0)\n",
    "print(f\"pct diff: {round((cont_us_only.at['from consump'] - cont_us_only.at['dry production'])/cont_us_only.at['dry production'], 5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "supp_dry_comparison.to_csv(f'GIM comparison of dry production vs modeled production by state data year {data_year} for {max_iter} iterations ({supp_fract_prod_timestamp}).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
